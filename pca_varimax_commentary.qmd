---
title: "New Insights into PCA + Varimax for Psychological Researchers"
author: "Florian Pargent, Timo von Oertzen, & David Goretzko"
format: pdf
bibliography: references.bib
csl: elsevier-vancouver.csl
editor: visual
---

```{r}
#| echo: false
#| message: false
phonedata_items = read.csv2("datasets/Items.csv")
phonedata_items = na.omit(phonedata_items[, 3:302])
phonedata_sensing = readRDS(file = "datasets/clusterdata.RDS")
phonedata_sensing = phonedata_sensing[, c(1:1821)]
# phonedata_sensing = phonedata_sensing[, 
#   sapply(phonedata_sensing, function(x) !anyNA(x))]
```

## Old Version

Rohe & Zeng (R&Z; @rohe2022vintage) proof what many psychologists have suspected but is dismissed by the literature: Principal component analysis in combination with Varimax rotation (PCA+VMR\; @kaiser1958varimax) *does* estimate factor scores in latent variable models. The factor indeterminacy problem which plagues VMR since its invention only applies for the special case of (multivariate) normally distributed factors. For any other distribution, perfect factor indeterminacy does not apply, although identifiability might be weak. However, distributions producing factors which are sparse or have a high kurtosis fulfill a *sufficient* leptokurtic condition, which enables PCA+VMR to estimate factors, and can be confirmed by simple diagnostics [@thurstone1947multiple].

As psychologists, we appreciate new insights into “vintage” PCA+VMR, which is still widely used for developing and evaluating psychological tests and questionnaires [@fokkema2017how]. From our experience, it is frustrating to teach factor analysis and rotational indeterminacy, followed by demonstrations of the effectiveness of PCA+VMR in analyzing the structure of psychological questionnaires. Despite practical success, the literature has fought against the use of PCA+VMR in favor of more complex exploratory factor analysis and oblique rotation methods. R&Z show that simple PCA+VMR performs the same inference. However, their technical paper is difficult to read for psychological researchers, which is why we summarize the most important aspects in this commentary:

In contrast to the examples in R&Z which only deal with sparse binary network data, psychological applications (traditionally) deal with i) questionnaire items or (increasingly) ii) digital data, e.g., mobile sensing. The $A$ matrix consists of i) integer-valued responses on $d$ Likert items or ii) continuous values on $d$ sensing variables, by $n$ persons. Degree normalization discussed by R&Q seems not suitable here and z-standardization is often required to detect meaningful factors in practice. We also do not share R&Z’s enthusiasm that "radial streaks" are common. In our online materials (<https://osf.io/5symf/>), we analyzed personality items ($n =`r nrow(phonedata_items)`$, $d =`r ncol(phonedata_items)`$; @stachl2020predicting). Although PCA+VMR was quite effective (with and without z-standardization) to detect the theoretically implied Big Five factors, no radial streaks were detected. When analyzing sensing variables in the same dataset ($n =`r nrow(phonedata_sensing)`$, $d =`r ncol(phonedata_sensing)`$), radial streaks were strong without z-standardization, but the factors were not interpretable. Interpretable factors were only detected with z-standardization, in which case radial streaks were much weaker.

Another important aspect of psychological applications are correlated factors (e.g. personality dimensions conscientiousness and neuroticism are thought to correlate around $-0.4$; @vanderlinden2010general), thus oblique rotations are often argued for. Interestingly, R&Z hint that in their semi-parametric factor model, their extension of PCA+VMR might be used to also estimate correlated factors. Our online materials demonstrate that $\hat{Z}\hat{B}$ correctly estimates correlated factors simulated from a leptokurtic distribution.

While R&Z’s results are highly relevant for widespread applications of PCA+VMR, details might differ in psychology which could profit from better understanding of the $B$ matrix, and how to estimate oblique factors and loadings.

## New Version

As psychologists, we appreciate Rohe & Zeng's (R&Z; @rohe2022vintage) new insights into “vintage” principal component analysis with varimax rotation (PCA+VMR).
Theories of intelligence and personality, perhaps psychology's contributions best known outside of our field, have been a direct product of PCA.
PCA+VRM is still widely used for developing and evaluating psychological tests and questionnaires, although the literature has fought against it in favor of more complex factor analytic techniques [@fokkema2017how].

In our opinion, abandoning the simpler PCA(+VRM) is a mistake and R&Z refute a common argument by proving that PCA+VRM *can* perform statistical inference in latent variable models:
The factor indeterminacy problem which plagued VMR since its invention only applies for the special case of normally distributed factors.
For any other distribution, perfect factor indeterminacy does not apply, although identifiability might be weak.
However, distributions producing sparse components fulfill a *sufficient* leptokurtic condition, which can be confirmed by simple diagnostics.

Because the results are complicated, we relate them to psychological applications.
The examples in R&Z only deal with sparse binary network data, but in typical psychological applications, the $A$ matrix consists of responses of $n$ persons to $d$ items which are either binary (intelligence tests), integer-valued (personality questionnaires) or continuous (digital sensors).
Psychologists are often interested in whether i) items can be structured in a simple way to represent a small number of meaningful components, and ii) those components can be interpreted as psychological constructs that describe interindividual differences.
R&Z show that "radial streaks" in the rotated loading matrix $\hat{Y}$ suggest that item loadings are identified and can be estimated with PCA+VRM from the data.
Similarly, streaks in the component matrix $\hat{Z}$ suggest that person scores can be estimated.

However, we question whether streaks are common in psychology with regard to both aspects.
<!-- Test and questionnaire items are traditionally designed to measure only a single component, so "simple structure" reflected by streaks in $\hat{Y}$ might be expected. -->
<!-- Psychological constructs are often conceptualized as roughly normally distributed, so streaks in $\hat{Z}$ seem more questionable. -->
In our online materials (<https://osf.io/5symf/>), we analyze a dataset [@stachl2020predicting] containing both personality items ($n =`r nrow(phonedata_items)`$, $d =`r ncol(phonedata_items)`$) and smartphone sensing variables ($n =`r nrow(phonedata_sensing)`$, $d =`r ncol(phonedata_sensing)`$).
Streaks were found only in $\hat{Y}$ but not in $\hat{Z}$.
It is also a cautionary example how imputation of missing values in combination with inappropriate data processing seems to produce streaks in $\hat{Z}$ that belong to uninterpretable components.
<!-- Degree normalization as discussed in R&Q is not suitable for many psychological datasets and other procedures like z-standardization are often required to detect meaningful factors. -->
Finally, we demonstrate R&Z's surprising side result that the matrix $\hat{Z}\hat{B}$ from PCA+VRM can correctly estimate person scores simulated from oblique leptokurtic components.
<!-- Finally, we demonstrate R&Z's surprising side result that PCA+VRM can sometimes correctly estimate correlated person scores. -->

<!-- Although R&Z proof that latent variable models sometimes could be estimated by PCA+VRM, the method not necessarily has its main use in this way. -->
In our opinion, the main usefulness of PCA+VRM not necessarily stems from its ability to estimate latent variable models.
PCA excels at providing meaningful descriptions in practical applications but R&Z's and our examples also show that there is rarely a single definite structure.
<!-- We think that components' main usefulness stems from their ability to predict or explain other meaningful quantities, which does not require any latent variables in a strict sense. -->
Components are most usefulness when they predict other meaningful quantities, which does not require latent variables in any strict sense.

### References

::: {#refs}
:::
